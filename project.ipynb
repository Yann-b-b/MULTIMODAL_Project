{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 49999/50000 [02:52<00:00, 290.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000 samples to omega_50k_samples.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset in streaming mode\n",
    "dataset = load_dataset(\"omegalabsinc/omega-multimodal\", split=\"train\", streaming=True)\n",
    "\n",
    "# Number of samples to collect\n",
    "num_samples = 50000\n",
    "output_file = \"omega_50k_samples.json\"\n",
    "\n",
    "# Stream and save data\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, sample in tqdm(enumerate(dataset), total=num_samples, desc=\"Saving Dataset\"):\n",
    "        json.dump(sample, f)\n",
    "        f.write(\"\\n\")  # New line for each JSON object\n",
    "        if i >= num_samples - 1:\n",
    "            break\n",
    "\n",
    "print(f\"Saved {num_samples} samples to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ **Sample 1 Field Types:**\n",
      "  - video_id: <class 'str'>\n",
      "  - youtube_id: <class 'str'>\n",
      "  - description: <class 'str'>\n",
      "  - views: <class 'int'>\n",
      "  - start_time: <class 'int'>\n",
      "  - end_time: <class 'int'>\n",
      "  - video_embed: <class 'list'>\n",
      "  - audio_embed: <class 'list'>\n",
      "  - description_embed: <class 'list'>\n",
      "  - description_relevance_score: <class 'float'>\n",
      "  - query_relevance_score: <class 'float'>\n",
      "  - query: <class 'str'>\n",
      "  - submitted_at: <class 'int'>\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "ðŸ”¹ **Sample 2 Field Types:**\n",
      "  - video_id: <class 'str'>\n",
      "  - youtube_id: <class 'str'>\n",
      "  - description: <class 'str'>\n",
      "  - views: <class 'int'>\n",
      "  - start_time: <class 'int'>\n",
      "  - end_time: <class 'int'>\n",
      "  - video_embed: <class 'list'>\n",
      "  - audio_embed: <class 'list'>\n",
      "  - description_embed: <class 'list'>\n",
      "  - description_relevance_score: <class 'float'>\n",
      "  - query_relevance_score: <class 'float'>\n",
      "  - query: <class 'str'>\n",
      "  - submitted_at: <class 'int'>\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "ðŸ”¹ **Sample 3 Field Types:**\n",
      "  - video_id: <class 'str'>\n",
      "  - youtube_id: <class 'str'>\n",
      "  - description: <class 'str'>\n",
      "  - views: <class 'int'>\n",
      "  - start_time: <class 'int'>\n",
      "  - end_time: <class 'int'>\n",
      "  - video_embed: <class 'list'>\n",
      "  - audio_embed: <class 'list'>\n",
      "  - description_embed: <class 'list'>\n",
      "  - description_relevance_score: <class 'float'>\n",
      "  - query_relevance_score: <class 'float'>\n",
      "  - query: <class 'str'>\n",
      "  - submitted_at: <class 'int'>\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "ðŸ”¹ **Sample 4 Field Types:**\n",
      "  - video_id: <class 'str'>\n",
      "  - youtube_id: <class 'str'>\n",
      "  - description: <class 'str'>\n",
      "  - views: <class 'int'>\n",
      "  - start_time: <class 'int'>\n",
      "  - end_time: <class 'int'>\n",
      "  - video_embed: <class 'list'>\n",
      "  - audio_embed: <class 'list'>\n",
      "  - description_embed: <class 'list'>\n",
      "  - description_relevance_score: <class 'float'>\n",
      "  - query_relevance_score: <class 'float'>\n",
      "  - query: <class 'str'>\n",
      "  - submitted_at: <class 'int'>\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "ðŸ”¹ **Sample 5 Field Types:**\n",
      "  - video_id: <class 'str'>\n",
      "  - youtube_id: <class 'str'>\n",
      "  - description: <class 'str'>\n",
      "  - views: <class 'int'>\n",
      "  - start_time: <class 'int'>\n",
      "  - end_time: <class 'int'>\n",
      "  - video_embed: <class 'list'>\n",
      "  - audio_embed: <class 'list'>\n",
      "  - description_embed: <class 'list'>\n",
      "  - description_relevance_score: <class 'float'>\n",
      "  - query_relevance_score: <class 'float'>\n",
      "  - query: <class 'str'>\n",
      "  - submitted_at: <class 'int'>\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to saved dataset\n",
    "dataset_path = \"omega_50k_samples.json\"\n",
    "\n",
    "# Load and inspect dataset\n",
    "def inspect_dataset_structure(file_path, num_samples=5):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i in range(num_samples):\n",
    "            line = f.readline().strip()\n",
    "            if not line:\n",
    "                print(f\"ðŸ”´ Empty line detected at entry {i+1}\")\n",
    "                continue\n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                print(f\"\\nðŸ”¹ **Sample {i+1} Field Types:**\")\n",
    "                for key, value in sample.items():\n",
    "                    print(f\"  - {key}: {type(value)}\")\n",
    "                print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"ðŸ”´ JSON Decode Error in line {i+1}\")\n",
    "\n",
    "# Run preview function\n",
    "inspect_dataset_structure(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "  video_embed size: 1024\n",
      "  description_embed size: 1024\n",
      "  audio_embed size: 1024\n",
      "========================================\n",
      "Sample 2:\n",
      "  video_embed size: 1024\n",
      "  description_embed size: 1024\n",
      "  audio_embed size: 1024\n",
      "========================================\n",
      "Sample 3:\n",
      "  video_embed size: 1024\n",
      "  description_embed size: 1024\n",
      "  audio_embed size: 1024\n",
      "========================================\n",
      "Sample 4:\n",
      "  video_embed size: 1024\n",
      "  description_embed size: 1024\n",
      "  audio_embed size: 1024\n",
      "========================================\n",
      "Sample 5:\n",
      "  video_embed size: 1024\n",
      "  description_embed size: 1024\n",
      "  audio_embed size: 1024\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dataset_path = \"omega_50k_samples.json\"\n",
    "\n",
    "def inspect_vector_sizes(file_path, num_samples=5):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i in range(num_samples):\n",
    "            line = f.readline().strip()\n",
    "            if not line:\n",
    "                print(f\"Empty line at sample {i+1}\")\n",
    "                continue\n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                video_size = len(sample.get(\"video_embed\", []))\n",
    "                description_size = len(sample.get(\"description_embed\", []))\n",
    "                audio_size = len(sample.get(\"audio_embed\", []))\n",
    "                print(f\"Sample {i+1}:\")\n",
    "                print(f\"  video_embed size: {video_size}\")\n",
    "                print(f\"  description_embed size: {description_size}\")\n",
    "                print(f\"  audio_embed size: {audio_size}\")\n",
    "                print(\"=\"*40)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"JSON Decode Error in sample {i+1}\")\n",
    "\n",
    "inspect_vector_sizes(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset:  11%|â–ˆ         | 5568/50000 [00:03<00:27, 1636.99it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define MLP model for modality transformation\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=2028):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Custom dataset class for loading preprocessed embeddings\n",
    "class OmegaMultimodalDataset(Dataset):\n",
    "    def __init__(self, file_path, num_samples=50000):\n",
    "        self.data = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in tqdm(enumerate(f), total=num_samples, desc=\"Loading Dataset\"):\n",
    "                sample = json.loads(line)\n",
    "\n",
    "                # Ensure all required fields are present\n",
    "                if \"audio_embed\" in sample and \"description_embed\" in sample and \"video_embed\" in sample:\n",
    "                    self.data.append({\n",
    "                        \"audio\": torch.tensor(sample[\"audio_embed\"], dtype=torch.float32),\n",
    "                        \"text\": torch.tensor(sample[\"description_embed\"], dtype=torch.float32),\n",
    "                        \"video\": torch.tensor(sample[\"video_embed\"], dtype=torch.float32),\n",
    "                    })\n",
    "\n",
    "                if len(self.data) >= num_samples:\n",
    "                    break\n",
    "\n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(\"ðŸ”´ No valid samples found! Check JSON structure.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"omega_50k_samples.json\"\n",
    "dataset = OmegaMultimodalDataset(dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Determine embedding size\n",
    "embedding_dim = len(dataset[0][\"audio\"])\n",
    "\n",
    "# Initialize MLP heads for modality transformation\n",
    "audio_to_text = MLPHead(embedding_dim, embedding_dim)\n",
    "text_to_video = MLPHead(embedding_dim, embedding_dim)\n",
    "video_to_audio = MLPHead(embedding_dim, embedding_dim)\n",
    "\n",
    "# Move models to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_to_text.to(device)\n",
    "text_to_video.to(device)\n",
    "video_to_audio.to(device)\n",
    "\n",
    "# Optimizers for each MLP\n",
    "optimizer_a2t = optim.Adam(audio_to_text.parameters(), lr=1e-4)\n",
    "optimizer_t2v = optim.Adam(text_to_video.parameters(), lr=1e-4)\n",
    "optimizer_v2a = optim.Adam(video_to_audio.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function (Cosine Similarity Loss)\n",
    "criterion = nn.CosineEmbeddingLoss(margin=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_a2t = 0\n",
    "    total_loss_t2v = 0\n",
    "    total_loss_v2a = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move data to device\n",
    "        audio_emb = batch[\"audio\"].to(device)\n",
    "        text_emb = batch[\"text\"].to(device)\n",
    "        video_emb = batch[\"video\"].to(device)\n",
    "\n",
    "        # Forward passes through MLPs\n",
    "        pred_text = audio_to_text(audio_emb)  # Audio â†’ Text\n",
    "        pred_video = text_to_video(pred_text)  # Text â†’ Video\n",
    "        pred_audio = video_to_audio(pred_video)  # Video â†’ Audio\n",
    "\n",
    "        # Define positive labels for CosineEmbeddingLoss\n",
    "        positive_labels = torch.ones(audio_emb.shape[0]).to(device)\n",
    "\n",
    "        # Compute individual losses per MLP\n",
    "        loss_a2t = criterion(pred_text, text_emb, positive_labels)  # Audio â†’ Text Loss\n",
    "        loss_t2v = criterion(pred_video, video_emb, positive_labels)  # Text â†’ Video Loss\n",
    "        loss_v2a = criterion(pred_audio, audio_emb, positive_labels)  # Video â†’ Audio Loss\n",
    "\n",
    "        # Zero gradients for each optimizer\n",
    "        optimizer_a2t.zero_grad()\n",
    "        optimizer_t2v.zero_grad()\n",
    "        optimizer_v2a.zero_grad()\n",
    "\n",
    "        # Backpropagate individual losses\n",
    "        loss_a2t.backward()\n",
    "        loss_t2v.backward()\n",
    "        loss_v2a.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer_a2t.step()\n",
    "        optimizer_t2v.step()\n",
    "        optimizer_v2a.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss_a2t += loss_a2t.item()\n",
    "        total_loss_t2v += loss_t2v.item()\n",
    "        total_loss_v2a += loss_v2a.item()\n",
    "\n",
    "    # Compute average loss per epoch\n",
    "    avg_loss_a2t = total_loss_a2t / len(dataloader)\n",
    "    avg_loss_t2v = total_loss_t2v / len(dataloader)\n",
    "    avg_loss_v2a = total_loss_v2a / len(dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  ðŸ”¹ Audio â†’ Text Loss: {avg_loss_a2t:.4f}\")\n",
    "    print(f\"  ðŸ”¹ Text â†’ Video Loss: {avg_loss_t2v:.4f}\")\n",
    "    print(f\"  ðŸ”¹ Video â†’ Audio Loss: {avg_loss_v2a:.4f}\")\n",
    "\n",
    "# Save trained models\n",
    "torch.save(audio_to_text.state_dict(), \"audio_to_text.pth\")\n",
    "torch.save(text_to_video.state_dict(), \"text_to_video.pth\")\n",
    "torch.save(video_to_audio.state_dict(), \"video_to_audio.pth\")\n",
    "\n",
    "print(\"âœ… Models saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume audio_embed, text_embed, video_embed are 2D arrays of shape (N, 1024) for raw embeddings\n",
    "# and audio_embed_aligned, text_embed_aligned, video_embed_aligned for transformed embeddings.\n",
    "# Combine embeddings for UMAP (stack in order: audio, text, video)\n",
    "raw_embeddings = np.vstack([audio_embed, text_embed, video_embed])\n",
    "aligned_embeddings = np.vstack([audio_embed_aligned, text_embed_aligned, video_embed_aligned])\n",
    "\n",
    "# Create modality labels for color-coding\n",
    "labels = (['Audio'] * len(audio_embed) + \n",
    "          ['Text'] * len(text_embed) + \n",
    "          ['Video'] * len(video_embed))\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 2D UMAP projection for raw and aligned embeddings\n",
    "umap_2d = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)  # :contentReference[oaicite:2]{index=2}\n",
    "raw_2d = umap_2d.fit_transform(raw_embeddings)\n",
    "aligned_2d = umap_2d.fit_transform(aligned_embeddings)  # separate fit to see new arrangement\n",
    "\n",
    "# Plot side-by-side 2D scatterplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "modalities = ['Audio', 'Text', 'Video']\n",
    "colors = {'Audio': 'red', 'Text': 'green', 'Video': 'blue'}\n",
    "\n",
    "for ax, data, title in zip(axes, [raw_2d, aligned_2d], ['Raw Embeddings (UMAP 2D)', 'Aligned Embeddings (UMAP 2D)']):\n",
    "    for mod in modalities:\n",
    "        idx = (labels == mod)\n",
    "        ax.scatter(data[idx, 0], data[idx, 1], s=10, color=colors[mod], label=mod, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3D UMAP projection for raw and aligned embeddings\n",
    "umap_3d = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "raw_3d = umap_3d.fit_transform(raw_embeddings)\n",
    "aligned_3d = umap_3d.fit_transform(aligned_embeddings)\n",
    "\n",
    "# Plot 3D scatter for raw vs. aligned (in separate figures for clarity)\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for mod in modalities:\n",
    "    idx = (labels == mod)\n",
    "    ax.scatter(raw_3d[idx, 0], raw_3d[idx, 1], raw_3d[idx, 2], s=15, color=colors[mod], label=mod, alpha=0.7)\n",
    "ax.set_title('Raw Embeddings (UMAP 3D)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for mod in modalities:\n",
    "    idx = (labels == mod)\n",
    "    ax.scatter(aligned_3d[idx, 0], aligned_3d[idx, 1], aligned_3d[idx, 2], s=15, color=colors[mod], label=mod, alpha=0.7)\n",
    "ax.set_title('Aligned Embeddings (UMAP 3D)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 2D t-SNE projection for raw and aligned embeddings\n",
    "tsne = TSNE(n_components=2, perplexity=30, init='pca', random_state=42)\n",
    "raw_tsne_2d = tsne.fit_transform(raw_embeddings)      # :contentReference[oaicite:4]{index=4}\n",
    "aligned_tsne_2d = tsne.fit_transform(aligned_embeddings)\n",
    "\n",
    "# Plot side-by-side 2D t-SNE scatterplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for ax, data, title in zip(axes, [raw_tsne_2d, aligned_tsne_2d], ['Raw Embeddings (t-SNE 2D)', 'Aligned Embeddings (t-SNE 2D)']):\n",
    "    for mod in modalities:\n",
    "        idx = (labels == mod)\n",
    "        ax.scatter(data[idx, 0], data[idx, 1], s=10, color=colors[mod], label=mod, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Optional) For 3D t-SNE, set n_components=3 in TSNE() and use a 3D scatter plot as done for UMAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute cosine distance matrices for raw and aligned embeddings\n",
    "cos_sim_raw = cosine_similarity(raw_embeddings)\n",
    "cos_sim_aligned = cosine_similarity(aligned_embeddings)\n",
    "dist_matrix_raw = 1 - cos_sim_raw   # cosine distance = 1 - cosine similarity&#8203;:contentReference[oaicite:6]{index=6}\n",
    "dist_matrix_aligned = 1 - cos_sim_aligned\n",
    "\n",
    "# Define a common color scale for fair comparison\n",
    "max_val = max(dist_matrix_raw.max(), dist_matrix_aligned.max())\n",
    "min_val = min(dist_matrix_raw.min(), dist_matrix_aligned.min())  # (should be ~0)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Raw embeddings distance heatmap\n",
    "sns.heatmap(dist_matrix_raw, ax=axes[0], vmin=min_val, vmax=max_val, cmap=\"YlGnBu\", \n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'label': 'Cosine Distance'})\n",
    "axes[0].set_title('Pairwise Distances (Raw)')\n",
    "\n",
    "# Aligned embeddings distance heatmap\n",
    "sns.heatmap(dist_matrix_aligned, ax=axes[1], vmin=min_val, vmax=max_val, cmap=\"YlGnBu\", \n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'label': 'Cosine Distance'})\n",
    "axes[1].set_title('Pairwise Distances (Aligned)')\n",
    "\n",
    "# (Optional) Add white lines to separate modality blocks for clarity\n",
    "N_a, N_t, N_v = len(audio_embed), len(text_embed), len(video_embed)\n",
    "for ax in axes:\n",
    "    ax.axvline(N_a, color='white'); ax.axvline(N_a+N_t, color='white')\n",
    "    ax.axhline(N_a, color='white'); ax.axhline(N_a+N_t, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
